---
title: "Derivative in Python and Colab"
date: 2022-07-10T16:02:08-07:00
---

hi everyone in this video i'm going to be talking about the derivatives as this applies to for example deep learning now derivative is important in the field like deep learning is because a lot of time in in deep learning we want to maximize a model or another way to say that we want to minimize the error function and the error function the way to minimize that is we need to know how the path of the function is moving is it increasing or decreasing and the derivative tell us about that essentially the derivative uh show how a function changes uh which is with respect to the horizontal variable and you know you can think of the roof there but the slope of the function at each point right there so so yeah with derivative and you have the error function um it can tell uh the direction that the path that you need to follow in order to minimize the error in other words maximize the the value for the model right here in order to in order to do derivative in colab and python we also need to import the senpai library right here this one right here is a plotting sim lot that we can use this blood the function for the uh created from simply now you can see right here right just for an example um a polynomial the derivative of a polynomial function is defined as this right here so if if the polynomial has the x to the n power then derivative is n to the minus one and you bring n down here and i create this function formula using latex in between the two money symbols right here as you can see with the text cell

so to create um a function in simpli we need we need to define the symbol first right here using uh the simple function in simpli to define x and then after that we create the function and this one right here is 7 times x to the third power and to differentiate to find the derivative of that we use the def function right here we pass in the uh the symbol and the function and then here we'll print it out um the function and the derivative derivative this plot right here is just gonna plot the function and the derivative for minus 5 to 5 right here and then here's the title for each of the plot okay so if i run this you see that it's going to give us the um the function the function the derivative and the plot of the function and the plot of the derivative right here you can see um over here is increasing slowly and then faster so the slope is going to be positive but the slope is going to be positive and um and incline steeper right there and then you can see right here right is is approaching zero it's approaching zero and it's decreasing it's decrea um it's increasing to zero in this situation right here too but slowing down the pace up but the pace of decreasing so that's why you see the slope is the slope is left become less and less and approaching zero right now okay and then another few example and for derivative is the sigmoid sigmoid function and the rayleigh function they are important because they they are used many times in the neural network as activation function okay now activation function is if you have an input right and you have a weighted node right there the activation function will transform the input and according to the way and the activation function here as a transformation and usually the the sigmoid or the rayleigh function right here okay now that this right here is the graph of the sigmoid function i'm using the mat.lip right here and then here's the formula for the sigmoid function uh basically it's one over this entire expression right here x bar exponential to the minus x okay so we plot that you can see right there it's kind of horizontal like this increasing into zero and then kind of tapering up at one right here okay

and here is the derivative of the sigmoid function is basically derivative that is just you know sigmoid time one minus sigma right here so if you plot it you can see that it's kind of have a normal gaussian distribution right here one thing you also want to notice is that the maximum number of derivative is 0.25 as compared to the sigmoid right here which is one so if you plot it on the same graph it's going to look smaller like this right here now finally we also get to the uh

relu and the regular function is that s0 when x is negative and increasing linearly when x is positive right here so you know i plot in this um in the cell right here i create a number of equal space 1000 equal number between minus 10 and 10 and then i compute the rayleigh using this function here maximum of zero or x so when x is minus it becomes zero so here we plot x and y and give it a tidal radius right here and then for the for the derivative you can see right here right when x is less than zero it's going to be zero and then for this right here when x is uh uh positive it's going to be one so this one you know this one right here is bonding x and plotting that condition right there so these are some of the examples of derivative you know starting with the simple polynomial to the activation sigmoid and relu i hope you found this helpful you know but please give a thumb up give a like give a hug subscribe and have a good day

{{< youtube AI1l_mVT7UA >}} 


[Google Colab Notebook of Derivative](https://colab.research.google.com/drive/1G9JMBQVvEvJgZ05UxpP6LN-H9lh5fBYo?usp=sharing) . 


![Colab Picture 1 of Derivative](/img/derivative-01.jpg)
![Colab Picture 2 of Derivative](/img/derivative-02.jpg)
![Colab Picture 3 of Derivative](/img/derivative-03.jpg)
![Colab Picture 4 of Derivative](/img/derivative-04.jpg)