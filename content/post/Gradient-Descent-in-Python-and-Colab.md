---
title: "Gradient Descent in Python and Colab"
date: 2022-07-11T14:34:22-07:00
---

hi everyone in this video i'm going to be discussing about gradient descent in python using google code now radium descent is important in optimization in machine learning and in deep learning the reason is because a lot of time we have a model and we want to optimize the model or another way of saying that is you know we have and cost function on error function and we want to minimize that and the derivative or the gradient can tell us the direction that the terrain of the model is moving is it moving up is it moving down okay so if you take a look at the notebook right here you see that as i mentioned gradient descent is important in optimization deep learning and machine learning and we can follow the gradient to total minimum because we want to minimize the loss or error function right here and this minimum usually depends on the starting point so if you have two or three local minimum depending on your starting point it might go to one of the local minimum right here and it and the intuition behind the gradient is that the gradients give the direction of increasing that of the function increase the most so if we take small step in the opposite direction you know so minus the gradient then that might decrease the function each time and after each uh iteration giving a learning rate and giving the f of time or the number of time that we go through um you know the uh the computation process it might give a drivers like uh give us the local minimum so to give an example of the um to give an example of the gradient descent right here i created a function as you can see right here so i'm i actually import numpy mat dot lift senpai and the python display simpli is to do algebra and uh in python my book is to plot the numpy is to generate data okay and then the display right here just to display the equation uh in a nicer way so i define i'm using this right here is the is the way that we define function function in python right here with the def and the colon right here

and so this function returned this polynomial right here if you expand it now this polynomial is to the sixth power as you can see right here okay so this right here is expanded version of this one right here so if you expand it out and you take the derivative of the of this function right here you would get this right here so for example uh 6 x to the 6 right here right the derivative of that is 6 times x to the 6 minus 1 is 5 right here another example is right here a minus 14 to the third power that would become 32 so 3 times 14 and x to the um uh

uh 42 right here right x to the second power right here so this right here is the derivative of this one right here the second cell right here on the next cell plot the the two func the two function functions the derivative of the function and the function itself right here so this guy right here plot the function and discovery hip plot derivative of the function and for the x i'm using the linear space right here and basically you know if you look at the parameters in here it would create 1 000 equally spaced points uh starting from minus 3 to 2.5 so that that's what limits spay in numpy does okay and then for each one of this point right here i calculate the function the function of x or you know y in this situation and the derivative of that and i plot that onto both of them onto this block right here i give us a title plot of the function and this derivative and here's the x label here is the y label right here i'm turning the grid on so you can see there's a grid right there and then the legend is going to tell which you know which curve which fun curve belongs to what right here so f sub x is the blue and f primes of x derivative is kept oranges right here okay and then also you can see i also set the limit the minus 50 to 50 and -3 on the y-axis and x-limit is -3 to 2.5 so that it looks a bit nicer right here as you can see right here right here is a function it has a local minimum about two point minus 2.5 it has another local minimum about two right here and then you have a point right here where it's not a minimum or maximum but the derivative is 0 right here so about -1 right here and the orange line is the derivative of the function

you can also you know instead of just high coding this right here right you can also use senpai to express the equation the polynomial right here um and you can see right here right so i create a symbol x equals symbol x and then i define a function as same as before and assign it to the variable f of x right here okay and then you can see right here right with this you can use simpli to expand f of x into a different format uh this one right here this is the expand version of the same function and then you can use the diff uh diff function of simpli to differentiate you know pass in the function and then you can display the derivative i'll again i also use expand to so that i expand this derivative into um the format that we uh familiar to uh uh we seemed similar to what we seen

now uh this one right here right you cannot compute um uh directly you yeah using this um using this this and the this variable right here if you want to compute in your new computation you have to use the function lambda phi you have to use the function lambda file right here um and then but i assign that function to for the lambda5 version of this and then derivative of fung sim as uh as the lambda 5 version of this right here and you can see right here right if you print out function 3.1 using the previous definition uh function defined regularly in python and then you use a sim uh a lambda five version uh let's give us the same result right here fire weight okay now for the plotting for example i decide you know and this and this plot right here right i'm going to use this the uh simple version of function and the def uh a simple version of the derivative of the function and plot it right here notice that i also have to recreate the linspace again right here right because i'm using it's not a good practice but i'm using x you can see right here right i'm using x the again and at in this moment right here i change it to the type simply belong to supply right so if i if i didn't do this again right here and i put i pass x in here it's going to give us an error so i want to you know read kind of redundant but defined in space again as i did previously uh right here okay so when you do that and you pass it in uh you notice that uh you know uh the simple version of the plot uh give us the same result as this version right here okay

now when it comes to gradient descent we gonna have to define uh two new variables the training epoch and the learning rate and the training epoch is basically how many iteration we're gonna go through the algorithm and in the situation i'm defining it to be of i'm sending it to be a thousand now the learning rate is is to scaling the number and the step that we take in so if you have a small small number like this the step is going to be scale small in this situation i'm sending the learning rate to 0.001 and then the mintam is basically your initial starting point right here choosing from uh in this situation right here i'm taking my the range of x which is about a thousand right here um and then i'm going to pick one number out of that and i'm going to assign it to minterm uh just to review right you see that there's a function right here that's the minimum and there's another minimum local minimum this is actually the global minimum and then there's a point right here that's not a minimum but you have a stub which is orange line right here zero and depending on where your starting point your mintam is going to be uh it's gonna either pick this point this point at this point right here so

when i run the you know it previously uh it's yeah it's starting the starting point is 1.7 so it's going to pick the local minimum of 2 right here okay and the algorithm is essentially um we we start with the temp uh starting point and then we'll you know we'll take the derivative we'll take the derivative of that and multiply by the learning rate and then whatever the direction it is we gonna set it to the opposite direction this minus size right here and then we update our outpoint and we iterate again through this um this for loop right here uh from one on the way to a thousand right here and then at the end we start this is the starting point this is the ending point and this right here is the derivative of the ending point right here so if i run it again for example um it's gonna start at minus point a and it's gonna pick this point right here so minus point eight so start right here so i was going to pick this point somewhere right about here as low as the local that the thing is local minimum so let me try that again and again i see right here right if you pick minus 1.3 minus 1.3 is over here then you're going to go to this one right here and then let me do it one more time

and then if you if you start at one for example right here right and that's gonna go to this two right here uh as soon as a minimum so depending on the starting point um you're gonna get different results um finally we can also do this using the simpli approach right here this one this algorithm right here we use a regular function d5 function and then this one right here we're using the simple defined function to lamdify and you can see right here right this one right here using senpai and if i run it it's similar structure if i run it um you see that it's gonna give us more or less the same process so for example if you start at 2.4

and then 2.4 is right here it's going to find this minimum right here and you see that the derivative of that is essentially 0 10 to the minus 13 is really small number and 10 to the minus 14 is the really small number so here is the algorithm using the simple method as you can see right here and then here is the algorithm the gradient descent approach uh using the regular function right there so both of them would work

i hope that uh it helped you and you learned something about gradient descent uh if you do please give it a hug give it a like in a comment subscribe and have a good day

{{< youtube 0LtXsEASGTE >}}


[Google Colab Notebook of Gradient Descent](https://colab.research.google.com/drive/1T_fhuR7wESt6sTw0wSz8OJukM8Vnk8ml?usp=sharing) . 


![Colab Picture 1 of Gradient Descent](/img/gradient-01.jpg)
![Colab Picture 2 of Gradient Descent](/img/gradient-02.jpg)
![Colab Picture 3 of Gradient Descent](/img/gradient-03.jpg)
![Colab Picture 4 of Gradient Descent](/img/gradient-04.jpg)
![Colab Picture 5 of Gradient Descent](/img/gradient-05.jpg)
