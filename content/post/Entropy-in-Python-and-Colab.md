---
title: "Entropy in Python and Colab"
date: 2022-07-08T17:33:21-07:00
---

hi everyone in this video I'm going to be talking about entropy as is applied into for example deep learning or machine learning now entropy has made a definition for example entropy and physic describes the study of order or Disorder so the universe our universe have a natural tendency to go from a state of order to a state of disorder so that right there is one definition of entropy but entropy in this particular situation is to Define in information according to information Theory so if you take a look at the Google collab right here well first of all right we're gonna import the libraries that we're gonna need numpy torch and neural network functional as F right here so that we can perform the calculation of entropy later so entropy in the information Theory measures the price or uncertainty about a particular variable so like for example X so H is greater when the probability is around 0.5 percent and then least when it's closer to zero one so when the new one is more predictability then p is uh then eight is smaller and so when when you have a high H would mean that P is around 0.5 right here there's more variability um in your system and then when that when H is low uh there's more repeating data so that's more redundancy in there the the formula for entropy is defined as the the probable probability of event time the log of that event and then um and then you sum over on the event that can happen the you know in that particular model for example right there right the minus right here is because log uh probability Which is less than one lock has a number less than one is going to be minus so you could want to have a minus um and minus this one right here so that become positive uh okay there's also a thing called cross entropy oh by the way uh if you take a look at this right here right uh I'm using for collab I'm I'm creating this cell right here using a text cell and then here you see that the uh just say that there's the money symbol right here and I'm using using latex notation to generate this formula right here so if you want you can Google um you know latex in collab by just latex and put a question right right there and you can generate you know formulas like this and in collab now there's also another definition of cross entropy between two distribution like P and Q right here right so the cross entropy of the p and Q uh of the probability distribution p and Q describe the relationship between the two so for example if you have a model that is predicting a cat or another cat for example right there all right and um and then the the prick the prediction the Deep learning prediction if you are a probability some probability right then you can do the cross entropy to Mark the performance of the model Q in relationship with the p which is a cat another cat for example right there and across entropy formula is a p and Q is uh P time log Q right here and sum over on the B event so take a look taking a look at this

implementation uh cross entropy in collab in Python uh let's say right if you have an initial list of point a and point two point a is occurring and point two is not occurring the sum has to be equal to one and then if you run that for example right there right it's going to give you about 0.5 now if you for example um change it to a probability of 0.5 right here you run that you see that it has the highest number entropy highest when p is closer to 0.5 right there okay so let me uh change it back to point a right there and run that so now you see right here that H so to calculate entropy and collab we're just going to go to a for Loop and use this formula right here then you know go to the list first is point a calculate the entropy of that and then point two calculator that and then add them together and print it out okay another way to another way to calculate entropy is to use list comprehension right here right and then you sum over on the elements so this one right here just go to the list one by one calculate the entropy for each particular element in the list and then sum it over and you run that you see that it's still going to give you the same number you can also write it out explicitly as the binary event right here so here is the probability of you know occurring my plus the entropy of not occurring add them together and print it out okay to calculate the cross entropy you can use the formula above right here and this is this is implementation and collab okay so imagine here it's a probability of a cat another cat right there right and then here's a model predict point a and point two so when you do that right there you can calculate across entropy p and Q right here B and Q so it's going to go to the length which is two right here the length of the list and let's calculate the cross entropy uh element zero and then element one right here and as is printed out um uh the cross entropy writing finally you can also do cross entropy in Python right there right the first thing you need to do is you need to convert the ND array to the tensor right there right because uh tensor doesn't play uh sometimes doesn't play nice with the order the variable type so when you convert it to tensor right there right you can put it use the function binary cross entropy right here and then one thing you want to pay attention to is if you want to put if you want to do PQ right you have to put Q first and P stack and like this right here and when you do that it's going to give you the same number as the the one using numpy all right this is so this is a brief introduction to entropy and cross entropy in Python and collab I hope this helps you if you like the videos uh please give a thumb up give a up subscribe and have a good day

{{< youtube rI2PtQF3juc >}} 


[Google Colab Notebook of Logarithm](https://colab.research.google.com/drive/15nEZPP9aGvLPONzxzWa3xUxKd0fQRhBo?usp=sharing) . 


![Colab Picture 1 of Entropy](/img/entropy-01.jpg)
![Colab Picture 2 of Entropy](/img/entropy-02.jpg)
![Colab Picture 3 of Entropy](/img/entropy-03.jpg)
