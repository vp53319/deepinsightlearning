---
title: "Gradient Descent 2D in Python and Colab"
date: 2022-07-12T19:06:52-07:00
---

hi everyone in this video i'm going to be talking about gradient in the two-dimension function now in my other video we i discussed about gradient in one dimension but um you know in real life uh when we work on model we work on function that has higher dimension than one dimension for example you can have a surface with x and y coordinate right there that's so that's two dimensions right there and when you work in one dimension there might be some challenges for example when we find a local minimum or something like that it might not turn out to be a minimum at all it just happened to be the slope to be zero but in higher dimension that problem might be less prevalent than in one dimension and of course we also have in either dimension um we also have methodology to correct and to overcome those uh challenges right there if you take a look for example right here in this google collab and you can see that first of all i'm going to import numpy matt.liv for the random library and simply right here the method left is to plot

the function the numpy is to generate the data point as a random is also for example when we start our initial starting point to descend then we we could want to use a random starting point and simpli is to uh to do algebra uh to to make the equation uh symbolically okay now another thing you also want to i also put here is that here is the symbol for the partial derivative here is the gradient the partial derivative is for example if you have a graph like a function like this right here what it means is that if you take the partial derivative with respect to x does mean that you keep everything else constant you keep y constant and you just take normal derivative like this right here and the gradient is just the partial uh of all of the variables so if you have two variables x and y then you do a partial derivative of x and partial derivative of y so this is a gradient right here and i

i do i make this function using latex and for collab we just use text cell we create text cell we put the question mark right here and then we do the the symbol within the question mark right here so for example this one right here is the partial derivative with respect to x okay and for example in this first in this first example right here right we're just going to plot this function right here which is x square minus y square and you know from the space minus 2 to 2 x and y right here and we're going to do a 3d plot right here you can see right here right and

you notice that for example at this point right here right both of the derivative partial derivative give us kind of a zero slope right here for example but in you know entire dimension right here i just wouldn't consider the minimum for example so some of the challenges that we encounter uh in one dimension uh it might not be so in two-dimension okay as also mentioned that there's other technique that can uh this challenge just if we run into them and um if you take a look at this graph right here right this graph right here this block right 3d block right here is of the function x square minus y square it's pretty simple um you know the minimum is going to be 0 0 and everything else is going to be bigger than the the minimum right there right so we can see that the minimum should be 0 0 and any gradient descent should go to this point right here

now to do the to do the gradient descent we can define a function

and this is how you define a function in python left with a colon and this one so i'm taking two values x and y is going to return x squared plus y square right here and then for the partial derivative you notice that it's more or less the same with both of x and y so the partial derivative with respect to x is 2 times 2 times x to the one power right here right so for this i'm just using one definition one function definition for both partial derivative of x and y just put in the point return two time points right here partial derivative now to do the to do the um uh gradient descent right we're gonna specify the learning rate and the epoch epoch is just basically the number iteration is gonna go and the learning rate is gonna scaling the step each step that is the how uh how long how wide how small the step is taking right here so in this example i'm setting the epoch to be 1000 and the learning rate is 0.01 so i'm going to start at this point right here i'm going to start two random couple start a random starting point um using the random function uniform function of the random library and it's basically going to pick a random part between minus two and two put in the x direction um put uh put another random format between minus two and two and put in the y position and so here is the ten right here and so this one right here is just a starting point okay and then i'm going to go through the algorithm in this for loop right here uh basically from 0 to 1000 and the gradient is basically just the partial derivative okay um this one right here is x zero your starting point zero uh for python and one is the second part y so radian is partial derivative with respect to x and then this one here partial derivative with respect to y so when we found a gradient we updating our output and in the algorithm basically minus is going the opposite of the uh the slope if the slope point positive you can equal minus if the slope is minus you can put positive right here so this one right here is updating the x position the zero and this one right here is updating the second position right here and then it's going to print out the final position now this one right here is a simple example right so wherever you start it's going to drive eventually to the point zero zero right here okay so let me run it again see right there right this is practically zero this particular zero so anyway you know this minus 1.8 and point a is zero zero and there's also zero zero zero zero right here so so for a more challenging example we can take a look at this equation right here x times e exponent exponential exponent to the minus x square minus y square right here and here is a plot of this equation using the range uh minus uh four minus two to two at every point one uh increment right here right and by the way you can notice that i have to convert x and y into the mass grid right here right and the mass grid is basically uh a fun uh measure for the mass grid function is used to create a rectangular grid out of two given one-dimensional array right there and it's necessary to plot to do that to plot in 3d right here okay so

nothing's noticing that this function right here right you can see that it has a minimum of somewhere where x this is x uh where x is going to be about 0.707 and y is going to be equal to 0 right here okay and we can look at this like we can compute this using simpli so i could create both more symbols x and y um right here okay and then i can create a function z using the symbol right here and then here's the exponent exponent uh x squared minus y squared minus x times this whole thing right here in order to do the calculation we have to do the um the lambda phi and use the lambda 5 function right here to lambda to lambda 5 partial the derivative of x plus the derivative of y and here is an example of the calculation print now the function right here partial

x partial y at this point right here and you notice you know you notice also for example right if you do the partial derivative of x partial derivative of y here are the results of that and you notice that for example the product product rule so if you're going to do the derivative with respect to x for example right here right you have this time the derivative of this which is the 2 times x to the 1 right here right and x and if you bring it out like this it could be -2 x square right here and then it would be the next one would be this time the derivative of this which is one which is this one right here okay and then the root for with respect to y is pretty simple it's just going to be uh 2y right here okay and multiply by the whole expression right here and then i as i mentioned earlier you can do the calculation uh you can put in the number and do the calculation right here okay yeah and you can you can use the sole function right here to solve for the stationary point with respect to partial derivative of x and positive derivative of y so

yeah you can see right here what i printed out the point is that if when x is equal to square root of 2 over 2 either plus or minus then this one right here is going to be the opposite of this expression right here so just be zero and then for the uh for the partial derivative of y right either x 0 or y 0 would give you the zero slope right there okay so for the training for example right here right it's going to be a little bit more uh i would say a bit more complicated uh complicated than the one-dimensional array uh once a one-dimensional radian but the the constant you know the concept because the conceptual step uh is just the same uh for example right here right we start a point at this guy and this guy right here but we pick a random point between minus 2 and 2 give it to x minus 2 and 2 give it to y and then over here right we calculate the gradient with respect to x right here and this one right here is a gradient with respect to y right we store that value in the gradient variable and then we update our initial position using the learning grade and the gradient right here with a minus sign and then we print out at the end we put now the final step right here so if you have a starting point somewhere in here somewhere in here it's going to drive you here but if you have a starting point somewhere in here you know it's not going to be able to find a minimum right there so that's one that's one thing about a gradient descent is that in a way it kind of depends on your starting point so if i run the example again right here right you notice that if i start i started on the minus sign x to the minus sign you'll be able to find you know the uh the approximation close enough to zero and to uh square root of two over two but if i started the on the other side because notice that it does not give us that the the correct result so here's an example of gradient descent in two-dimension i hope that you find it helpful and educational if you do but please if i like give a hug give a thumb up comment thank you and have a good day

{{< youtube GE4qmS8qcNc >}}


[Google Colab Notebook of Gradient Descent](https://colab.research.google.com/drive/1ei79nQo-eKN0hl7ym6fBXkyyZ-b6wgbx?usp=sharing) . 


![Colab Picture 1 of Gradient Descent](/img/gradient2d-01.jpg)
![Colab Picture 2 of Gradient Descent](/img/gradient2d-02.jpg)
![Colab Picture 3 of Gradient Descent](/img/gradient2d-03.jpg)
![Colab Picture 4 of Gradient Descent](/img/gradient2d-04.jpg)
![Colab Picture 5 of Gradient Descent](/img/gradient2d-05.jpg)
